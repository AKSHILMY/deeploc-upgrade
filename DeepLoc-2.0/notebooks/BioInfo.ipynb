{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9KOFSFVqyJf",
        "outputId": "4eeaae94-3341-47e0-9204-6c73c876ce5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deeploc_path = '/content/drive/MyDrive/DeepLoc-2.0'"
      ],
      "metadata": {
        "id": "qkB6pOtLqyma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "s9ijGIIWr29u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(deeploc_path)"
      ],
      "metadata": {
        "id": "vOHoGnQKr7a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Current working directory:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGOUNde1r-Zx",
        "outputId": "d810ce99-174b-48cb-a305-b5b85a505eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content/drive/.shortcut-targets-by-id/1-AX-1APxko2ETix81_EIK1-GxoBB94yn/DeepLoc-2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd1zBS8LsAPI",
        "outputId": "88eff922-054e-463b-a00e-636d738e12f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.11.4)\n",
            "Collecting Bio (from -r requirements.txt (line 5))\n",
            "  Downloading bio-1.7.1-py3-none-any.whl (280 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.0/281.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.3.0+cu121)\n",
            "Collecting onnxruntime>=1.7.0 (from -r requirements.txt (line 7))\n",
            "  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fair-esm==0.4.0 (from -r requirements.txt (line 8))\n",
            "  Downloading fair_esm-0.4.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (4.41.2)\n",
            "Collecting pytorch_lightning (from -r requirements.txt (line 10))\n",
            "  Downloading pytorch_lightning-2.3.1-py3-none-any.whl (812 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (0.1.99)\n",
            "Collecting pickle5 (from -r requirements.txt (line 12))\n",
            "  Downloading pickle5-0.0.11.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.1/132.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (3.9.0)\n",
            "Collecting pandas (from -r requirements.txt (line 3))\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 3)) (2023.4)\n",
            "Collecting biopython>=1.80 (from Bio->-r requirements.txt (line 5))\n",
            "  Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gprofiler-official (from Bio->-r requirements.txt (line 5))\n",
            "  Downloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting mygene (from Bio->-r requirements.txt (line 5))\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from Bio->-r requirements.txt (line 5)) (1.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Bio->-r requirements.txt (line 5)) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from Bio->-r requirements.txt (line 5)) (4.66.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.7.0->-r requirements.txt (line 7))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.7.0->-r requirements.txt (line 7)) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.7.0->-r requirements.txt (line 7)) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 9)) (0.23.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 9)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 9)) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 9)) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 9)) (0.4.3)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning->-r requirements.txt (line 10))\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities>=0.10.0 (from pytorch_lightning->-r requirements.txt (line 10))\n",
            "  Downloading lightning_utilities-0.11.3.post0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 15)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 15)) (3.5.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec->torch->-r requirements.txt (line 6)) (3.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning->-r requirements.txt (line 10)) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 2)) (1.16.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.7.0->-r requirements.txt (line 7))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 6)) (2.1.5)\n",
            "Collecting biothings-client>=0.2.6 (from mygene->Bio->-r requirements.txt (line 5))\n",
            "  Downloading biothings_client-0.3.1-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio->-r requirements.txt (line 5)) (4.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Bio->-r requirements.txt (line 5)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Bio->-r requirements.txt (line 5)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Bio->-r requirements.txt (line 5)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Bio->-r requirements.txt (line 5)) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 6)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 6)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 6)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 6)) (4.0.3)\n",
            "Building wheels for collected packages: pickle5\n",
            "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle5: filename=pickle5-0.0.11-cp310-cp310-linux_x86_64.whl size=255318 sha256=9ab5d604a5b98fd68671acfc01f06be93d248c5378a019bb1858687ab6afb802\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/14/ef/4aab19d27fa8e58772be5c71c16add0426acf9e1f64353235c\n",
            "Successfully built pickle5\n",
            "Installing collected packages: pickle5, fair-esm, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, humanfriendly, biopython, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gprofiler-official, coloredlogs, biothings-client, onnxruntime, nvidia-cusolver-cu12, mygene, Bio, torchmetrics, pytorch_lightning\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Bio-1.7.1 biopython-1.84 biothings-client-0.3.1 coloredlogs-15.0.1 fair-esm-0.4.0 gprofiler-official-1.0.0 humanfriendly-10.0 lightning-utilities-0.11.3.post0 mygene-3.2.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 onnxruntime-1.18.1 pandas-1.5.3 pickle5-0.0.11 pytorch_lightning-2.3.1 torchmetrics-1.4.0.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_sl.py --model Fast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7dC3p4qsDcG",
        "outputId": "eed9b3fe-f497-483d-883e-b58468ae7e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing embeddings\n",
            "Training subcellular localization models\n",
            "Training model 1 / 5\n",
            "Training model 2 / 5\n",
            "Training model 3 / 5\n",
            "/usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "2024-06-29 11:12:06.495542: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-29 11:12:06.495592: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-29 11:12:06.596398: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-29 11:12:06.609042: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-29 11:12:07.885628: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /content/drive/.shortcut-targets-by-id/1-AX-1APxko2ETix81_EIK1-GxoBB94yn/DeepLoc-2.0/models/models_esm1b exists and is not empty.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name       | Type          | Params | Mode \n",
            "-----------------------------------------------------\n",
            "0 | initial_ln | LayerNorm     | 2.6 K  | train\n",
            "1 | lin        | Linear        | 327 K  | train\n",
            "2 | attn_head  | AttentionHead | 768    | train\n",
            "3 | clf_head   | Linear        | 2.8 K  | train\n",
            "4 | kld        | KLDivLoss     | 0      | train\n",
            "-----------------------------------------------------\n",
            "334 K     Trainable params\n",
            "0         Non-trainable params\n",
            "334 K     Total params\n",
            "1.336     Total estimated model params size (MB)\n",
            "Epoch 0:  25% 160/632 [09:02<26:41,  3.39s/it, v_num=1]/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "Epoch 0: 100% 632/632 [16:15<00:00,  1.54s/it, v_num=1]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/64 [00:00<?, ?it/s]       \u001b[A\n",
            "Validation DataLoader 0:   0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 20/64 [00:07<00:16,  2.72it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 40/64 [00:15<00:09,  2.55it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 60/64 [00:26<00:01,  2.27it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 64/64 [00:30<00:00,  2.13it/s]\u001b[A\n",
            "Epoch 1: 100% 632/632 [05:16<00:00,  2.00it/s, v_num=1]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/64 [00:00<?, ?it/s]       \u001b[A\n",
            "Validation DataLoader 0:   0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 20/64 [00:07<00:16,  2.60it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 40/64 [00:16<00:09,  2.47it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 60/64 [00:27<00:01,  2.22it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 64/64 [00:29<00:00,  2.15it/s]\u001b[A\n",
            "Epoch 1: 100% 632/632 [05:46<00:00,  1.82it/s, v_num=1]`Trainer.fit` stopped: `max_epochs=2` reached.\n",
            "Epoch 1: 100% 632/632 [05:47<00:00,  1.82it/s, v_num=1]\n",
            "Training model 4 / 5\n",
            "/usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: models/models_esm1b/3_1Layer/lightning_logs\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /content/drive/.shortcut-targets-by-id/1-AX-1APxko2ETix81_EIK1-GxoBB94yn/DeepLoc-2.0/models/models_esm1b exists and is not empty.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name       | Type          | Params | Mode \n",
            "-----------------------------------------------------\n",
            "0 | initial_ln | LayerNorm     | 2.6 K  | train\n",
            "1 | lin        | Linear        | 327 K  | train\n",
            "2 | attn_head  | AttentionHead | 768    | train\n",
            "3 | clf_head   | Linear        | 2.8 K  | train\n",
            "4 | kld        | KLDivLoss     | 0      | train\n",
            "-----------------------------------------------------\n",
            "334 K     Trainable params\n",
            "0         Non-trainable params\n",
            "334 K     Total params\n",
            "1.336     Total estimated model params size (MB)\n",
            "Epoch 0: 100% 631/631 [05:27<00:00,  1.93it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/64 [00:00<?, ?it/s]       \u001b[A\n",
            "Validation DataLoader 0:   0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 20/64 [00:07<00:17,  2.58it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 40/64 [00:16<00:10,  2.37it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 60/64 [00:28<00:01,  2.13it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 64/64 [00:30<00:00,  2.07it/s]\u001b[A\n",
            "Epoch 1: 100% 631/631 [05:23<00:00,  1.95it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/64 [00:00<?, ?it/s]       \u001b[A\n",
            "Validation DataLoader 0:   0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 20/64 [00:08<00:17,  2.49it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 40/64 [00:17<00:10,  2.29it/s]\u001b[A\n",
            "Validation DataLoader 0:  94% 60/64 [00:28<00:01,  2.09it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 64/64 [00:31<00:00,  2.04it/s]\u001b[A\n",
            "Epoch 1: 100% 631/631 [05:55<00:00,  1.78it/s, v_num=0]`Trainer.fit` stopped: `max_epochs=2` reached.\n",
            "Epoch 1: 100% 631/631 [05:56<00:00,  1.77it/s, v_num=0]\n",
            "Training model 5 / 5\n",
            "/usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: models/models_esm1b/4_1Layer/lightning_logs\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /content/drive/.shortcut-targets-by-id/1-AX-1APxko2ETix81_EIK1-GxoBB94yn/DeepLoc-2.0/models/models_esm1b exists and is not empty.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name       | Type          | Params | Mode \n",
            "-----------------------------------------------------\n",
            "0 | initial_ln | LayerNorm     | 2.6 K  | train\n",
            "1 | lin        | Linear        | 327 K  | train\n",
            "2 | attn_head  | AttentionHead | 768    | train\n",
            "3 | clf_head   | Linear        | 2.8 K  | train\n",
            "4 | kld        | KLDivLoss     | 0      | train\n",
            "-----------------------------------------------------\n",
            "334 K     Trainable params\n",
            "0         Non-trainable params\n",
            "334 K     Total params\n",
            "1.336     Total estimated model params size (MB)\n",
            "Epoch 0: 100% 636/636 [05:34<00:00,  1.90it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/65 [00:00<?, ?it/s]       \u001b[A\n",
            "Validation DataLoader 0:   0% 0/65 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 20/65 [00:07<00:17,  2.63it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 40/65 [00:16<00:10,  2.48it/s]\u001b[A\n",
            "Validation DataLoader 0:  92% 60/65 [00:26<00:02,  2.23it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 65/65 [00:30<00:00,  2.15it/s]\u001b[A\n",
            "Epoch 1: 100% 636/636 [05:16<00:00,  2.01it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/65 [00:00<?, ?it/s]       \u001b[A\n",
            "Validation DataLoader 0:   0% 0/65 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  31% 20/65 [00:07<00:17,  2.64it/s]\u001b[A\n",
            "Validation DataLoader 0:  62% 40/65 [00:16<00:10,  2.49it/s]\u001b[A\n",
            "Validation DataLoader 0:  92% 60/65 [00:26<00:02,  2.24it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 65/65 [00:29<00:00,  2.17it/s]\u001b[A\n",
            "Epoch 1: 100% 636/636 [05:46<00:00,  1.83it/s, v_num=0]`Trainer.fit` stopped: `max_epochs=2` reached.\n",
            "Epoch 1: 100% 636/636 [05:47<00:00,  1.83it/s, v_num=0]\n",
            "Finished training subcellular localization models\n",
            "Using trained models to generate outputs for signal prediction training\n",
            "Generating output for ensemble model 0\n",
            "684it [04:05,  2.79it/s]\n",
            "184it [01:21,  2.26it/s]\n",
            "Generating output for ensemble model 1\n",
            "698it [04:14,  2.74it/s]\n",
            "171it [01:16,  2.22it/s]\n",
            "Generating output for ensemble model 2\n",
            "695it [04:11,  2.76it/s]\n",
            "173it [01:17,  2.24it/s]\n",
            "Generating output for ensemble model 3\n",
            "693it [04:12,  2.75it/s]\n",
            "175it [01:20,  2.17it/s]\n",
            "Generating output for ensemble model 4\n",
            "699it [04:14,  2.75it/s]\n",
            "169it [01:16,  2.20it/s]\n",
            "Generated outputs! Can train sorting signal prediction now\n",
            "Computing subcellular localization performance on swissprot CV dataset\n",
            "[0.3715332  0.40307617 0.51362305 0.759375   0.41552734 0.68408203\n",
            " 0.76308594 0.51860352 0.4534668  0.60449219 0.68994141]\n",
            "Computing fold\n",
            "Computing fold\n",
            "Computing fold\n",
            "Computing fold\n",
            "Computing fold\n",
            "/content/drive/.shortcut-targets-by-id/1-AX-1APxko2ETix81_EIK1-GxoBB94yn/DeepLoc-2.0/src/metrics.py:154: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
            "  print(pd.DataFrame(output_dict).to_latex())\n",
            "\\begin{tabular}{llllllllllllllllllll}\n",
            "\\toprule\n",
            "{} &     NumLabels & NumLabelsTest &  ACC\\_membrane &  MCC\\_membrane &    ACC\\_subloc & HammLoss\\_subloc & Jaccard\\_subloc & MicroF1\\_subloc & MacroF1\\_subloc &     Cytoplasm &       Nucleus & Extracellular & Cell membrane & Mitochondrion &       Plastid & Endoplasmic reticulum & Lysosome/Vacuole & Golgi apparatus &    Peroxisome \\\\\n",
            "\\midrule\n",
            "0 &  1.27 pm 0.03 &  1.28 pm 0.04 &  0.87 pm 0.01 &  0.70 pm 0.01 &  0.52 pm 0.02 &    0.93 pm 0.00 &   0.67 pm 0.01 &   0.71 pm 0.01 &   0.63 pm 0.01 &  0.60 pm 0.01 &  0.65 pm 0.02 &  0.85 pm 0.03 &  0.63 pm 0.01 &  0.73 pm 0.03 &  0.90 pm 0.01 &          0.50 pm 0.02 &     0.22 pm 0.03 &    0.35 pm 0.06 &  0.46 pm 0.07 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n",
            "NumLabels             : 1.27 + 0.03\n",
            "NumLabelsTest         : 1.28 + 0.04\n",
            "ACC_membrane          : 0.87 + 0.01\n",
            "MCC_membrane          : 0.70 + 0.01\n",
            "ACC_subloc            : 0.52 + 0.02\n",
            "HammLoss_subloc       : 0.93 + 0.00\n",
            "Jaccard_subloc        : 0.67 + 0.01\n",
            "MicroF1_subloc        : 0.71 + 0.01\n",
            "MacroF1_subloc        : 0.63 + 0.01\n",
            "Cytoplasm             : 0.60 + 0.01\n",
            "Nucleus               : 0.65 + 0.02\n",
            "Extracellular         : 0.85 + 0.03\n",
            "Cell membrane         : 0.63 + 0.01\n",
            "Mitochondrion         : 0.73 + 0.03\n",
            "Plastid               : 0.90 + 0.01\n",
            "Endoplasmic reticulum : 0.50 + 0.02\n",
            "Lysosome/Vacuole      : 0.22 + 0.03\n",
            "Golgi apparatus       : 0.35 + 0.06\n",
            "Peroxisome            : 0.46 + 0.07\n",
            "1.27 + 0.03\n",
            "1.28 + 0.04\n",
            "0.87 + 0.01\n",
            "0.70 + 0.01\n",
            "0.52 + 0.02\n",
            "0.93 + 0.00\n",
            "0.67 + 0.01\n",
            "0.71 + 0.01\n",
            "0.63 + 0.01\n",
            "0.60 + 0.01\n",
            "0.65 + 0.02\n",
            "0.85 + 0.03\n",
            "0.63 + 0.01\n",
            "0.73 + 0.03\n",
            "0.90 + 0.01\n",
            "0.50 + 0.02\n",
            "0.22 + 0.03\n",
            "0.35 + 0.06\n",
            "0.46 + 0.07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_sl.py --model Accurate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vzh7HaWRsZSx",
        "outputId": "02be7a2a-9a03-462c-ade5-3ca87bb1061f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer_config.json: 100% 24.0/24.0 [00:00<00:00, 136kB/s]\n",
            "spiece.model: 100% 238k/238k [00:00<00:00, 19.9MB/s]\n",
            "special_tokens_map.json: 100% 1.79k/1.79k [00:00<00:00, 11.3MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 546/546 [00:00<00:00, 3.42MB/s]\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Embeddings not found, generating......\n",
            "pytorch_model.bin: 100% 11.3G/11.3G [01:17<00:00, 146MB/s]\n",
            "0it [00:01, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1-AX-1APxko2ETix81_EIK1-GxoBB94yn/DeepLoc-2.0/src/embedding.py\", line 45, in embed_prott5\n",
            "    embed = model(input_ids=torch.tensor(toks['input_ids'], device=device),\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\", line 1972, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\", line 1107, in forward\n",
            "    layer_outputs = layer_module(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\", line 687, in forward\n",
            "    self_attention_outputs = self.layer[0](\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\", line 594, in forward\n",
            "    attention_output = self.SelfAttention(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\", line 554, in forward\n",
            "    attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1885, in softmax\n",
            "    ret = input.softmax(dim)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.82 GiB. GPU \n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1-AX-1APxko2ETix81_EIK1-GxoBB94yn/DeepLoc-2.0/train_sl.py\", line 65, in <module>\n",
            "    generate_embeddings(model_attrs)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1-AX-1APxko2ETix81_EIK1-GxoBB94yn/DeepLoc-2.0/src/embedding.py\", line 66, in generate_embeddings\n",
            "    embed_prott5(embed_dataloader, EMBEDDINGS[model_attrs.model_type][\"embeds\"])\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1-AX-1APxko2ETix81_EIK1-GxoBB94yn/DeepLoc-2.0/src/embedding.py\", line 54, in embed_prott5\n",
            "    raise Exception(\"Failed to create embeddings\")\n",
            "Exception: Failed to create embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_ss.py --model Fast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoSCDWY7F-0T",
        "outputId": "219d2e4b-6fb0-477c-dc52-091f18b896d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training sorting signal type prediction models\n",
            "Training model 1 / 5\n",
            "(2599, 267) (2599, 9) (289, 267) (289, 9)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "Training clf 0\n",
            "Missing logger folder: models/models_esm1b/signaltype/0/lightning_logs\n",
            "2024-06-29 13:08:31.648460: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-29 13:08:31.648524: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-29 13:08:31.769562: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-29 13:08:31.997839: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-29 13:08:34.171244: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /content/drive/.shortcut-targets-by-id/1-AX-1APxko2ETix81_EIK1-GxoBB94yn/DeepLoc-2.0/models/models_esm1b/signaltype exists and is not empty.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name | Type   | Params | Mode \n",
            "----------------------------------------\n",
            "0 | ln1  | Linear | 8.6 K  | train\n",
            "1 | ln2  | Linear | 297    | train\n",
            "----------------------------------------\n",
            "8.9 K     Trainable params\n",
            "0         Non-trainable params\n",
            "8.9 K     Total params\n",
            "0.035     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 0: 100% 20/20 [00:00<00:00, 46.44it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 3/3 [00:00<00:00, 373.25it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:00<00:00, 157.84it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 3/3 [00:00<00:00, 262.83it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:00<00:00, 136.02it/s, v_num=0]`Trainer.fit` stopped: `max_epochs=2` reached.\n",
            "Epoch 1: 100% 20/20 [00:00<00:00, 122.95it/s, v_num=0]\n",
            "Training model 2 / 5\n",
            "(2800, 267) (2800, 9) (312, 267) (312, 9)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "Training clf 1\n",
            "Missing logger folder: models/models_esm1b/signaltype/1/lightning_logs\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /content/drive/.shortcut-targets-by-id/1-AX-1APxko2ETix81_EIK1-GxoBB94yn/DeepLoc-2.0/models/models_esm1b/signaltype exists and is not empty.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name | Type   | Params | Mode \n",
            "----------------------------------------\n",
            "0 | ln1  | Linear | 8.6 K  | train\n",
            "1 | ln2  | Linear | 297    | train\n",
            "----------------------------------------\n",
            "8.9 K     Trainable params\n",
            "0         Non-trainable params\n",
            "8.9 K     Total params\n",
            "0.035     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (21) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 0: 100% 21/21 [00:00<00:00, 156.80it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 3/3 [00:00<00:00, 376.72it/s]\u001b[A\n",
            "Epoch 1: 100% 21/21 [00:00<00:00, 156.98it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 3/3 [00:00<00:00, 380.88it/s]\u001b[A\n",
            "Epoch 1: 100% 21/21 [00:00<00:00, 140.75it/s, v_num=0]`Trainer.fit` stopped: `max_epochs=2` reached.\n",
            "Epoch 1: 100% 21/21 [00:00<00:00, 127.18it/s, v_num=0]\n",
            "Training model 3 / 5\n",
            "(2691, 267) (2691, 9) (299, 267) (299, 9)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "Training clf 2\n",
            "Missing logger folder: models/models_esm1b/signaltype/2/lightning_logs\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /content/drive/.shortcut-targets-by-id/1-AX-1APxko2ETix81_EIK1-GxoBB94yn/DeepLoc-2.0/models/models_esm1b/signaltype exists and is not empty.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name | Type   | Params | Mode \n",
            "----------------------------------------\n",
            "0 | ln1  | Linear | 8.6 K  | train\n",
            "1 | ln2  | Linear | 297    | train\n",
            "----------------------------------------\n",
            "8.9 K     Trainable params\n",
            "0         Non-trainable params\n",
            "8.9 K     Total params\n",
            "0.035     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (21) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 0: 100% 21/21 [00:00<00:00, 154.14it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 3/3 [00:00<00:00, 363.78it/s]\u001b[A\n",
            "Epoch 1: 100% 21/21 [00:00<00:00, 155.48it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 3/3 [00:00<00:00, 364.81it/s]\u001b[A\n",
            "Epoch 1: 100% 21/21 [00:00<00:00, 138.86it/s, v_num=0]`Trainer.fit` stopped: `max_epochs=2` reached.\n",
            "Epoch 1: 100% 21/21 [00:00<00:00, 127.04it/s, v_num=0]\n",
            "Training model 4 / 5\n",
            "(2714, 267) (2714, 9) (302, 267) (302, 9)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "Training clf 3\n",
            "Missing logger folder: models/models_esm1b/signaltype/3/lightning_logs\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /content/drive/.shortcut-targets-by-id/1-AX-1APxko2ETix81_EIK1-GxoBB94yn/DeepLoc-2.0/models/models_esm1b/signaltype exists and is not empty.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name | Type   | Params | Mode \n",
            "----------------------------------------\n",
            "0 | ln1  | Linear | 8.6 K  | train\n",
            "1 | ln2  | Linear | 297    | train\n",
            "----------------------------------------\n",
            "8.9 K     Trainable params\n",
            "0         Non-trainable params\n",
            "8.9 K     Total params\n",
            "0.035     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (21) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 0: 100% 21/21 [00:00<00:00, 107.24it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 3/3 [00:00<00:00, 201.73it/s]\u001b[A\n",
            "Epoch 1: 100% 21/21 [00:00<00:00, 107.05it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 3/3 [00:00<00:00, 266.56it/s]\u001b[A\n",
            "Epoch 1: 100% 21/21 [00:00<00:00, 96.54it/s, v_num=0]`Trainer.fit` stopped: `max_epochs=2` reached.\n",
            "Epoch 1: 100% 21/21 [00:00<00:00, 90.87it/s, v_num=0]\n",
            "Training model 5 / 5\n",
            "(2637, 267) (2637, 9) (293, 267) (293, 9)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "Training clf 4\n",
            "Missing logger folder: models/models_esm1b/signaltype/4/lightning_logs\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /content/drive/.shortcut-targets-by-id/1-AX-1APxko2ETix81_EIK1-GxoBB94yn/DeepLoc-2.0/models/models_esm1b/signaltype exists and is not empty.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name | Type   | Params | Mode \n",
            "----------------------------------------\n",
            "0 | ln1  | Linear | 8.6 K  | train\n",
            "1 | ln2  | Linear | 297    | train\n",
            "----------------------------------------\n",
            "8.9 K     Trainable params\n",
            "0         Non-trainable params\n",
            "8.9 K     Total params\n",
            "0.035     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 0: 100% 20/20 [00:00<00:00, 165.95it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 3/3 [00:00<00:00, 410.87it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:00<00:00, 150.32it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3 [00:00<?, ?it/s]        \u001b[A\n",
            "Validation DataLoader 0:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100% 3/3 [00:00<00:00, 393.92it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:00<00:00, 134.59it/s, v_num=0]`Trainer.fit` stopped: `max_epochs=2` reached.\n",
            "Epoch 1: 100% 20/20 [00:00<00:00, 120.97it/s, v_num=0]\n",
            "Finished training sorting signal type prediction models\n",
            "Using trained models to generate outputs of signal prediction\n",
            "Generating output for ensemble model 0\n",
            "Generating output for ensemble model 1\n",
            "Generating output for ensemble model 2\n",
            "Generating output for ensemble model 3\n",
            "Generating output for ensemble model 4\n",
            "Generated outputs!\n",
            "Computing sorting signal type prediction performance on swissprot CV dataset\n",
            "/content/drive/.shortcut-targets-by-id/1-AX-1APxko2ETix81_EIK1-GxoBB94yn/DeepLoc-2.0/src/metrics.py:192: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
            "  print(pd.DataFrame(output_dict).to_latex())\n",
            "\\begin{tabular}{lllllllllllll}\n",
            "\\toprule\n",
            "{} &       microF1 &       macroF1 &      accuracy &            SP &            TM &            MT &            CH &            TH &           NLS &           NES &           PTS &            GPI \\\\\n",
            "\\midrule\n",
            "0 &  0.65 pm 0.04 &  0.44 pm 0.10 &  0.41 pm 0.13 &  0.76 pm 0.04 &  0.18 pm 0.27 &  0.64 pm 0.34 &  0.27 pm 0.34 &  0.20 pm 0.26 &  0.56 pm 0.18 &  0.51 pm 0.18 &  0.63 pm 0.23 &  -0.00 pm 0.07 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PkRulMP1GiKe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}